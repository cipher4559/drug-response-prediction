# -*- coding: utf-8 -*-
"""Drug_Responce.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18u8cMQNGymkZXvSlZGrJ-ojRc_eEEq1I
"""

# ===============================
# 1. Install dependencies
# ===============================
!pip install imbalanced-learn shap joblib --quiet

# ===============================
# 2. Import libraries
# ===============================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import RandomOverSampler
import joblib
import shap
from google.colab import files

# ===============================
# 3. Upload dataset
# ===============================
uploaded = files.upload()  # Choose CSV file
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)

print("âœ… Dataset loaded successfully!")
print("Shape:", df.shape)
print(df.head())

# ===============================
# 4. Detect Target Column Automatically
# ===============================
possible_targets = [c for c in df.columns if 'response' in c.lower() or 'outcome' in c.lower() or 'label' in c.lower()]
if len(possible_targets) > 0:
    target_col = possible_targets[0]
    print(f"Detected target column automatically: {target_col}")
else:
    print("\nCould not auto-detect target column. Columns available:")
    print(df.columns)
    target_col = input("\nEnter the name of the target column exactly as shown above: ")

# ===============================
# 5. Basic EDA
# ===============================
print("\nMissing values per column:\n", df.isnull().sum())
print("\nClass distribution:\n", df[target_col].value_counts())

sns.countplot(x=target_col, data=df)
plt.title(f'{target_col} distribution')
plt.xticks(rotation=45)
plt.show()

# ===============================
# 6. Preprocessing Setup
# ===============================
X = df.drop(columns=[target_col])
y = df[target_col]

# Encode target labels
le = LabelEncoder()
y = le.fit_transform(y)
print("Class mapping:", dict(zip(le.classes_, le.transform(le.classes_))))

# Handle imbalance
class_counts = pd.Series(y).value_counts()
print("\nOriginal class counts:\n", class_counts)

if class_counts.min() / class_counts.max() < 0.5:
    print("\nâš ï¸ Detected imbalance. Applying RandomOverSampler...")
    ros = RandomOverSampler(random_state=42)
    X, y = ros.fit_resample(X, y)
    print("New class counts after oversampling:\n", pd.Series(y).value_counts())

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Define preprocessing pipeline
categorical_cols = X.select_dtypes(include=["object"]).columns.tolist()
numeric_cols = X.select_dtypes(exclude=["object"]).columns.tolist()

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
        ("num", "passthrough", numeric_cols)
    ]
)

# ===============================
# 7. Train Model (Random Forest)
# ===============================
rf_model = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        class_weight='balanced'
    ))
])

rf_model.fit(X_train, y_train)

# ===============================
# 8. Evaluate
# ===============================
y_pred = rf_model.predict(X_test)
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=le.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# ===============================
# 9. SHAP Explainability
# ===============================
# Transform test data with feature names
X_test_transformed = rf_model.named_steps["preprocessor"].transform(X_test)
feature_names = rf_model.named_steps["preprocessor"].get_feature_names_out()
X_test_transformed = pd.DataFrame(X_test_transformed, columns=feature_names)

# Build explainer on classifier only
explainer = shap.TreeExplainer(rf_model.named_steps["classifier"])
shap_values = explainer.shap_values(X_test_transformed)

# Plot SHAP summary plots for multi-class
shap.summary_plot(shap_values, X_test_transformed, plot_type="bar", class_names=le.classes_)
shap.summary_plot(shap_values, X_test_transformed, class_names=le.classes_)

# ===============================
# 10. Model Comparison
# ===============================
models = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=500, random_state=42, solver="liblinear"),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
}

print("\nðŸ” Model Comparison (5-Fold Cross-Validation)")
for name, model in models.items():
    pipe = Pipeline(steps=[("preprocessor", preprocessor), ("classifier", model)])
    scores = cross_val_score(pipe, X, y, cv=5, scoring="f1_macro")
    print(f"{name}: Mean F1 = {scores.mean():.4f} (+/- {scores.std():.4f})")

# ===============================
# 11. Save Model & Encoder
# ===============================
joblib.dump(rf_model, "drug_response_pipeline.pkl")
joblib.dump(le, "label_encoder.pkl")
print("âœ… Pipeline & encoder saved!")

# ===============================
# Install dependencies (if needed)
# ===============================
!pip install joblib gradio --quiet

# ===============================
# Import libraries
# ===============================
import pandas as pd
import joblib
import gradio as gr

# ===============================
# Load trained pipeline & encoder
# ===============================
pipeline = joblib.load("drug_response_pipeline.pkl")
le = joblib.load("label_encoder.pkl")

# ===============================
# Single Prediction Function
# ===============================
def predict_single(age, sex, tumor_size, biomarker_status):
    input_data = {
        "Age": age,
        "Tumor_Size": tumor_size,
        "Sex": sex.strip().title(),
        "Biomarker_Status": biomarker_status.strip().title(),
    }
    input_df = pd.DataFrame([input_data])

    # Prediction
    prediction_encoded = pipeline.predict(input_df)[0]
    prediction_label = le.inverse_transform([prediction_encoded])[0]
    return f"âœ… Predicted Drug Response: {prediction_label}"

# ===============================
# Batch Prediction Function
# ===============================
def predict_batch(file):
    batch_df = pd.read_csv(file.name)

    # Required columns
    required_cols = ["Age", "Tumor_Size", "Sex", "Biomarker_Status"]
    missing_cols = [c for c in required_cols if c not in batch_df.columns]
    if missing_cols:
        return f"âŒ Missing required columns: {missing_cols}"

    # Normalize categorical inputs
    batch_df["Sex"] = batch_df["Sex"].astype(str).str.strip().str.title()
    batch_df["Biomarker_Status"] = batch_df["Biomarker_Status"].astype(str).str.strip().str.title()

    # Predictions
    preds_encoded = pipeline.predict(batch_df)
    preds_labels = le.inverse_transform(preds_encoded)

    batch_df["Predicted_Response"] = preds_labels
    return batch_df

# ===============================
# Gradio Interface
# ===============================
with gr.Blocks() as demo:
    gr.Markdown("## ðŸ’Š Drug Response Prediction App")
    gr.Markdown("Predict drug response for cancer patients using a trained ML model.")

    # --- Single Prediction Tab ---
    with gr.Tab("ðŸ‘¤ Single Prediction"):
        age = gr.Number(label="Age", value=50)
        sex = gr.Radio(choices=["Male", "Female"], label="Sex", value="Male")
        tumor_size = gr.Number(label="Tumor Size (mm)", value=20.0)
        biomarker_status = gr.Radio(choices=["Positive", "Negative"], label="Biomarker Status", value="Positive")
        predict_btn = gr.Button("Predict Response")
        output_single = gr.Textbox(label="Prediction")

        predict_btn.click(
            fn=predict_single,
            inputs=[age, sex, tumor_size, biomarker_status],
            outputs=output_single
        )

    # --- Batch Prediction Tab ---
    with gr.Tab("ðŸ“‚ Batch Prediction"):
        file_input = gr.File(label="Upload CSV", file_types=[".csv"])
        output_batch = gr.Dataframe(label="Predictions")

        file_input.change(
            fn=predict_batch,
            inputs=file_input,
            outputs=output_batch
        )

# Launch the Gradio app
demo.launch()